{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be89c40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/22 14:18:23 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType, DoubleType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeExample\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Import delta after creating SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "import delta\n",
    "\n",
    "# Your existing code\n",
    "schema = StructType() \\\n",
    "    .add(\"ProductID\", IntegerType(), True) \\\n",
    "    .add(\"ProductName\", StringType(), True) \\\n",
    "    .add(\"Category\", StringType(), True) \\\n",
    "    .add(\"ListPrice\", DoubleType(), True)\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schema).load(\"products/products.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad47f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"managed_products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d745f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlQuery = \"DESCRIBE FORMATTED managed_products;\"\n",
    "df_spark = spark.sql(sqlQuery)\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e40780",
   "metadata": {},
   "source": [
    "## Use Delta tables for streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f56c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run notebookutils locally you need to install the dummy-notebookutils first: pip install dummy-notebookutils\n",
    "\n",
    "from notebookutils import mssparkutils\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a folder\n",
    "inputPath = 'Files/data/'\n",
    "mssparkutils.fs.mkdirs(inputPath)\n",
    "\n",
    "# Create a stream that reads data from the folder, using a JSON schema\n",
    "jsonSchema = StructType([\n",
    "StructField(\"device\", StringType(), False),\n",
    "StructField(\"status\", StringType(), False)\n",
    "])\n",
    "iotstream = spark.readStream.schema(jsonSchema).option(\"maxFilesPerTrigger\", 1).json(inputPath)\n",
    "\n",
    "# Write some event data to the folder\n",
    "device_data = '''{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
    "{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
    "{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
    "{\"device\":\"Dev2\",\"status\":\"error\"}\n",
    "{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
    "{\"device\":\"Dev1\",\"status\":\"error\"}\n",
    "{\"device\":\"Dev2\",\"status\":\"ok\"}\n",
    "{\"device\":\"Dev2\",\"status\":\"error\"}\n",
    "{\"device\":\"Dev1\",\"status\":\"ok\"}'''\n",
    "\n",
    "mssparkutils.fs.put(inputPath + \"data.txt\", device_data, True)\n",
    "\n",
    "print(\"Source stream created...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06900495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the stream to a delta table\n",
    "delta_stream_table_path = 'Tables/iotdevicedata'\n",
    "checkpointpath = 'Files/delta/checkpoint'\n",
    "deltastream = iotstream.writeStream.format(\"delta\").option(\"checkpointLocation\", checkpointpath).start(delta_stream_table_path)\n",
    "print(\"Streaming to delta sink...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e5239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more data to the source stream\n",
    "more_data = '''{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
    "{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
    "{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
    "{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
    "{\"device\":\"Dev1\",\"status\":\"error\"}\n",
    "{\"device\":\"Dev2\",\"status\":\"error\"}\n",
    "{\"device\":\"Dev1\",\"status\":\"ok\"}'''\n",
    "\n",
    "mssparkutils.fs.put(inputPath + \"more-data.txt\", more_data, True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
