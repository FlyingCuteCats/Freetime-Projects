{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fffe7d3",
   "metadata": {},
   "source": [
    "## Create the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae60f3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+-----------------+--------------------+--------------------+---+--------+--------+\n",
      "|    _c0|_c1|       _c2|              _c3|                 _c4|                 _c5|_c6|     _c7|     _c8|\n",
      "+-------+---+----------+-----------------+--------------------+--------------------+---+--------+--------+\n",
      "|SO43701|  1|2019-07-01|      Christy Zhu|christy12@adventu...|Mountain-100 Silv...|  1| 3399.99|271.9992|\n",
      "|SO43704|  1|2019-07-01|       Julio Ruiz|julio1@adventure-...|Mountain-100 Blac...|  1| 3374.99|269.9992|\n",
      "|SO43705|  1|2019-07-01|        Curtis Lu|curtis9@adventure...|Mountain-100 Silv...|  1| 3399.99|271.9992|\n",
      "|SO43700|  1|2019-07-01|     Ruben Prasad|ruben10@adventure...|  Road-650 Black, 62|  1|699.0982| 55.9279|\n",
      "|SO43703|  1|2019-07-01|   Albert Alvarez|albert7@adventure...|    Road-150 Red, 62|  1| 3578.27|286.2616|\n",
      "|SO43697|  1|2019-07-01|      Cole Watson|cole1@adventure-w...|    Road-150 Red, 62|  1| 3578.27|286.2616|\n",
      "|SO43699|  1|2019-07-01|    Sydney Wright|sydney61@adventur...|Mountain-100 Silv...|  1| 3399.99|271.9992|\n",
      "|SO43702|  1|2019-07-01|      Colin Anand|colin45@adventure...|    Road-150 Red, 44|  1| 3578.27|286.2616|\n",
      "|SO43698|  1|2019-07-01| Rachael Martinez|rachael16@adventu...|Mountain-100 Silv...|  1| 3399.99|271.9992|\n",
      "|SO43707|  1|2019-07-02|       Emma Brown|emma3@adventure-w...|    Road-150 Red, 48|  1| 3578.27|286.2616|\n",
      "|SO43711|  1|2019-07-02| Courtney Edwards|courtney1@adventu...|    Road-150 Red, 56|  1| 3578.27|286.2616|\n",
      "|SO43706|  1|2019-07-02|     Edward Brown|edward26@adventur...|    Road-150 Red, 48|  1| 3578.27|286.2616|\n",
      "|SO43708|  1|2019-07-02|        Brad Deng|brad2@adventure-w...|    Road-650 Red, 52|  1|699.0982| 55.9279|\n",
      "|SO43709|  1|2019-07-02|        Martha Xu|martha12@adventur...|    Road-150 Red, 52|  1| 3578.27|286.2616|\n",
      "|SO43710|  1|2019-07-02|     Katrina Raji|katrina20@adventu...|    Road-150 Red, 56|  1| 3578.27|286.2616|\n",
      "|SO43712|  1|2019-07-02|Abigail Henderson|abigail73@adventu...|    Road-150 Red, 44|  1| 3578.27|286.2616|\n",
      "|SO43720|  1|2019-07-03|  Melanie Sanchez|melanie47@adventu...|    Road-150 Red, 44|  1| 3578.27|286.2616|\n",
      "|SO43721|  1|2019-07-03|        Louis Xie|louis20@adventure...|    Road-150 Red, 62|  1| 3578.27|286.2616|\n",
      "|SO43714|  1|2019-07-03|   Latasha Alonso|latasha8@adventur...|    Road-150 Red, 44|  1| 3578.27|286.2616|\n",
      "|SO43715|  1|2019-07-03|       Warren Jai|warren42@adventur...|    Road-150 Red, 56|  1| 3578.27|286.2616|\n",
      "+-------+---+----------+-----------------+--------------------+--------------------+---+--------+--------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Because this is a local script, we need to create a Spark session.\n",
    "# In a Databricks notebook, the Spark session is created automatically. We only need to state %%pyspark at the top of the cell.\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadCSVExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"false\").load(\"orders/2019.csv\")\n",
    "df.show() # df.show() is a standard PySpark method, while display() is a method from Databricks notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af6bc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+----------+--------------+--------------------+--------------------+--------+---------+--------+\n",
      "|SalesOrderNumber|SalesOrderLineNumber| OrderDate|  CustomerName|               Email|                Item|Quantity|UnitPrice|     Tax|\n",
      "+----------------+--------------------+----------+--------------+--------------------+--------------------+--------+---------+--------+\n",
      "|         SO49171|                   1|2021-01-01| Mariah Foster|mariah21@adventur...|  Road-250 Black, 48|       1|2181.5625| 174.525|\n",
      "|         SO49172|                   1|2021-01-01|  Brian Howard|brian23@adventure...|    Road-250 Red, 44|       1|  2443.35| 195.468|\n",
      "|         SO49173|                   1|2021-01-01| Linda Alvarez|linda19@adventure...|Mountain-200 Silv...|       1|2071.4197|165.7136|\n",
      "|         SO49174|                   1|2021-01-01|Gina Hernandez|gina4@adventure-w...|Mountain-200 Silv...|       1|2071.4197|165.7136|\n",
      "|         SO49178|                   1|2021-01-01|     Beth Ruiz|beth4@adventure-w...|Road-550-W Yellow...|       1|1000.4375|  80.035|\n",
      "+----------------+--------------------+----------+--------------+--------------------+--------------------+--------+---------+--------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/30 15:24:07 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: orders/*.csv.\n",
      "java.io.FileNotFoundException: File orders/*.csv does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:100)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "orderSchema = StructType([\n",
    "    StructField(\"SalesOrderNumber\", StringType()),\n",
    "    StructField(\"SalesOrderLineNumber\", IntegerType()),\n",
    "    StructField(\"OrderDate\", DateType()),\n",
    "    StructField(\"CustomerName\", StringType()),\n",
    "    StructField(\"Email\", StringType()),\n",
    "    StructField(\"Item\", StringType()),\n",
    "    StructField(\"Quantity\", IntegerType()),\n",
    "    StructField(\"UnitPrice\", FloatType()),\n",
    "    StructField(\"Tax\", FloatType())\n",
    "])\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(orderSchema).load(\"orders/*.csv\")\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4564ccf9",
   "metadata": {},
   "source": [
    "## Filtering and Aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e384ee3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32718\n",
      "12427\n",
      "+--------------+--------------------+\n",
      "|  CustomerName|               Email|\n",
      "+--------------+--------------------+\n",
      "| Mariah Foster|mariah21@adventur...|\n",
      "|  Brian Howard|brian23@adventure...|\n",
      "| Linda Alvarez|linda19@adventure...|\n",
      "|Gina Hernandez|gina4@adventure-w...|\n",
      "|     Beth Ruiz|beth4@adventure-w...|\n",
      "+--------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "customers = df['CustomerName', 'Email']\n",
    "\n",
    "print(customers.count())\n",
    "print(customers.distinct().count())\n",
    "\n",
    "customers.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24144cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n",
      "133\n",
      "+----------------+--------------------+\n",
      "|    CustomerName|               Email|\n",
      "+----------------+--------------------+\n",
      "|    Margaret Guo|margaret24@advent...|\n",
      "|         Cara Xu|cara8@adventure-w...|\n",
      "|  Alejandro Raji|alejandro46@adven...|\n",
      "|      Jaime Diaz|jaime3@adventure-...|\n",
      "|Bridget Andersen|bridget15@adventu...|\n",
      "+----------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "customers = df.select(\"CustomerName\", \"Email\").where(df['Item']=='Road-250 Red, 52')\n",
    "print(customers.count())\n",
    "print(customers.distinct().count())\n",
    "\n",
    "customers.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d33abd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|                Item|sum(Quantity)|\n",
      "+--------------------+-------------+\n",
      "|Mountain-200 Blac...|          388|\n",
      "|Touring-1000 Yell...|           74|\n",
      "|Touring-1000 Blue...|           67|\n",
      "|Short-Sleeve Clas...|          216|\n",
      "|Women's Mountain ...|          146|\n",
      "+--------------------+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "productSales = df.select(\"Item\", \"Quantity\").groupBy(\"Item\").sum()\n",
    "\n",
    "productSales.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42d84aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|                Item|sum(Quantity)|\n",
      "+--------------------+-------------+\n",
      "|Mountain-200 Blac...|          388|\n",
      "|Touring-1000 Yell...|           74|\n",
      "|Touring-1000 Blue...|           67|\n",
      "|Short-Sleeve Clas...|          216|\n",
      "|Women's Mountain ...|          146|\n",
      "+--------------------+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "productSales = df.select(\"Item\", \"Quantity\").groupBy(\"Item\").sum()\n",
    "productSales.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c453b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Year|count|\n",
      "+----+-----+\n",
      "|2019| 1201|\n",
      "|2020| 2733|\n",
      "|2021|28784|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# This creates a new DataFrame with a single column called 'Year', which contains the year extracted from the 'OrderDate' column.\n",
    "\n",
    "yearlySales = df.select(year(col(\"OrderDate\")).alias(\"Year\")).groupBy(\"Year\").count().orderBy(\"Year\")\n",
    "\n",
    "yearlySales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9266ed8f",
   "metadata": {},
   "source": [
    "## Transform and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e614454d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+----------+----+-----+---------+---------+--------------------+--------------------+--------+---------+--------+\n",
      "|SalesOrderNumber|SalesOrderLineNumber| OrderDate|Year|Month|FirstName| LastName|               Email|                Item|Quantity|UnitPrice|     Tax|\n",
      "+----------------+--------------------+----------+----+-----+---------+---------+--------------------+--------------------+--------+---------+--------+\n",
      "|         SO49171|                   1|2021-01-01|2021|    1|   Mariah|   Foster|mariah21@adventur...|  Road-250 Black, 48|       1|2181.5625| 174.525|\n",
      "|         SO49172|                   1|2021-01-01|2021|    1|    Brian|   Howard|brian23@adventure...|    Road-250 Red, 44|       1|  2443.35| 195.468|\n",
      "|         SO49173|                   1|2021-01-01|2021|    1|    Linda|  Alvarez|linda19@adventure...|Mountain-200 Silv...|       1|2071.4197|165.7136|\n",
      "|         SO49174|                   1|2021-01-01|2021|    1|     Gina|Hernandez|gina4@adventure-w...|Mountain-200 Silv...|       1|2071.4197|165.7136|\n",
      "|         SO49178|                   1|2021-01-01|2021|    1|     Beth|     Ruiz|beth4@adventure-w...|Road-550-W Yellow...|       1|1000.4375|  80.035|\n",
      "+----------------+--------------------+----------+----+-----+---------+---------+--------------------+--------------------+--------+---------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Create Year and Month columns\n",
    "transformed_df = df.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\"Month\", month(col(\"OrderDate\")))\n",
    "\n",
    "# Create the new FirstName and LastName fields\n",
    "transformed_df = transformed_df.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n",
    "\n",
    "# Filter and reorder columns\n",
    "transformed_df = transformed_df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"Email\", \"Item\", \"Quantity\", \"UnitPrice\", \"Tax\"]\n",
    "\n",
    "# Display the first five orders\n",
    "transformed_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e9a6287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:>                                                         (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transformed_df.write.mode(\"overwrite\").parquet('Files/transformed_data/orders')\n",
    "\n",
    "print (\"Transformed data saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d7fe01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+----------+----+-----+---------+---------+--------------------+--------------------+--------+---------+--------+\n",
      "|SalesOrderNumber|SalesOrderLineNumber| OrderDate|Year|Month|FirstName| LastName|               Email|                Item|Quantity|UnitPrice|     Tax|\n",
      "+----------------+--------------------+----------+----+-----+---------+---------+--------------------+--------------------+--------+---------+--------+\n",
      "|         SO49171|                   1|2021-01-01|2021|    1|   Mariah|   Foster|mariah21@adventur...|  Road-250 Black, 48|       1|2181.5625| 174.525|\n",
      "|         SO49172|                   1|2021-01-01|2021|    1|    Brian|   Howard|brian23@adventure...|    Road-250 Red, 44|       1|  2443.35| 195.468|\n",
      "|         SO49173|                   1|2021-01-01|2021|    1|    Linda|  Alvarez|linda19@adventure...|Mountain-200 Silv...|       1|2071.4197|165.7136|\n",
      "|         SO49174|                   1|2021-01-01|2021|    1|     Gina|Hernandez|gina4@adventure-w...|Mountain-200 Silv...|       1|2071.4197|165.7136|\n",
      "|         SO49178|                   1|2021-01-01|2021|    1|     Beth|     Ruiz|beth4@adventure-w...|Road-550-W Yellow...|       1|1000.4375|  80.035|\n",
      "+----------------+--------------------+----------+----+-----+---------+---------+--------------------+--------------------+--------+---------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "orders_df = spark.read.format(\"parquet\").load(\"Files/transformed_data/orders\")\n",
    "orders_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37a487d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "orders_df.write.partitionBy(\"Year\",\"Month\").mode(\"overwrite\").parquet(\"Files/partitioned_data\")\n",
    "\n",
    "print (\"Transformed data saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ff5d2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/30 15:24:14 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: Files/partitioned_data/Year=2021/Month=*.\n",
      "java.io.FileNotFoundException: File Files/partitioned_data/Year=2021/Month=* does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:100)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+----------+---------+--------+--------------------+--------------------+--------+---------+-------+\n",
      "|SalesOrderNumber|SalesOrderLineNumber| OrderDate|FirstName|LastName|               Email|                Item|Quantity|UnitPrice|    Tax|\n",
      "+----------------+--------------------+----------+---------+--------+--------------------+--------------------+--------+---------+-------+\n",
      "|         SO59195|                   1|2021-11-01|   Alexia|   Hayes|alexia19@adventur...|Touring-2000 Blue...|       1|  1214.85| 97.188|\n",
      "|         SO59195|                   2|2021-11-01|   Alexia|   Hayes|alexia19@adventur...|     Racing Socks, M|       1|     8.99| 0.7192|\n",
      "|         SO59196|                   1|2021-11-01|  Anthony|  Garcia|anthony2@adventur...|Touring-2000 Blue...|       1|  1214.85| 97.188|\n",
      "|         SO59196|                   2|2021-11-01|  Anthony|  Garcia|anthony2@adventur...|Sport-100 Helmet,...|       1|    34.99| 2.7992|\n",
      "|         SO59191|                   1|2021-11-01|   Willie|    Nara|willie36@adventur...|Road-550-W Yellow...|       1|  1120.49|89.6392|\n",
      "+----------------+--------------------+----------+---------+--------+--------------------+--------------------+--------+---------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "orders_2021_df = spark.read.format(\"parquet\").load(\"Files/partitioned_data/Year=2021/Month=*\")\n",
    "\n",
    "orders_2021_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c7e998",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
